# Gradiant Descent
Cost Function $J(W,b)$를 최소화 시키는 방법.
![image](https://user-images.githubusercontent.com/101078168/175217365-63d9835c-18e5-4c8f-b1ea-9d271f23eb69.png)

$ W := W - a\frac{\partial J(W,b)}{\partial W}$

$ b := b - a\frac{\partial J(W,b)}{\partial b}$

> $a$ = learning rate
