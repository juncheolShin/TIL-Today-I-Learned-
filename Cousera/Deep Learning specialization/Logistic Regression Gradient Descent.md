# Logistic Regression Gradient Descent
------
$ z = w_1x_1 + w_2x_2 + b$   
$ a = sigmoid(z) $   
$L(a,y)$ 
------  
$dw_1 = x_1 Ã— dz$   
$dw_2 = x_2 Ã— dz$   
$db = dz$   
------   
$w_1 := w_1 - adw_1$   
$w_2 := w_2 - adw_2$           
$b := b - adb$
>$a$ = learning rate

*2022-06-24*
